\documentclass[handout]{beamer}

\usetheme{AnnArbor}\usecolortheme{beaver}

\usepackage{amsmath,verbatim,graphicx}

\title[Sparse signals]{Horseshoe prior}
\author{Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}


\newcommand{\I}{\mathrm{I}}

\begin{document}

\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{Literature}
\begin{itemize}
\item Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. "Handling sparsity via the horseshoe." International Conference on Artificial Intelligence and Statistics. 2009.
\item Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. "The horseshoe estimator for sparse signals." Biometrika 97.2 (2010): 465-480.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{A model for sparse signals}

Consider the model 
\[ y \sim N(\theta,\sigma^2 \I) \]
where $y$ and $\theta$ have dimension $p$\pause and where $\theta$ is \alert{sparse}, i.e. it has many zero (or nearly zero) entries.

\vspace{0.2in} \pause

Some well-known priors (penalities) for $\theta$ that encourage sparsity are 
\begin{itemize}
\item Normal (ridge)
\item t 
\item Laplace (LASSO)
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Normal (ridge) prior}

Consider 
\[ \begin{array}{rl}
y_i &\stackrel{ind}{\sim} N(\theta_i,1) \\
\theta_i &\stackrel{ind}{\sim} N(0,\lambda_i^2) 
\end{array} \]
\pause with $\lambda_i^2$ being unknown. \pause Then 
\[ E[\theta_i|y] = \int_0^1 (1-\kappa_i) y_i p(\kappa_i|y) d\kappa_i = (1-E[\kappa_i|y]) y_i \]
where $\kappa_i=1/(1+\lambda_i^2)$ \pause and $(1-E[\kappa_i|y])$ determines the amount of shrinkage toward 0 for data $y_i$. 

\vspace{0.2in} \pause

The normal prior sets $\lambda_i=\lambda$ and therefore $\kappa_i=\kappa$. \pause Regardless of the prior on $\lambda$, the same amount of shrinkage (as a percentage) will occur for all observations.
\end{frame}


\begin{frame}
\frametitle{Priors written as scale mixtures of normals}

Many common sparsity priors can be written as scale-mixtures of normals, i.e. 
\[ \theta_i\stackrel{ind}{\sim} N(0,\lambda_i^2) \]
where $\lambda_i$ has some distribution, e.g. 

\vspace{0.2in} \pause 

{\scriptsize
\begin{center}
\begin{tabular}{lll}
Prior for $\theta_i$ & Density for $\lambda_i$ & Density for $\kappa_i$ \\
\hline
Laplace (double-exponential) & $\lambda_i e^{-\lambda_i^2/2}$ & $\kappa_i^{-2}e^{-1/2\kappa_i}$ \\
Cauchy & $\lambda_i^{-2} e^{-1/2\lambda_i^2}$ & $\kappa_i^{-1/2} (1-\kappa_i)^{-3/2}e^{-\kappa_i/2(1-\kappa_i)}$ \\
Strawderman-Berger & $\lambda_i (1+\lambda_i^2)^{-3/2}$ & $\kappa_i^{-1/2}$ \\
Normal-exponential-gamma & $\lambda_i (1+\lambda_i^2)^{-(c+1)}$ & $\kappa_i^{c-1}$ \\
Normal-Jeffreys $(1/|\theta_i|)$ & $1/\lambda_i$ & $\kappa_i^{-1}(1-\kappa_i)^{-1}$ \\
Horseshoe & $(1+\lambda_i^2)^{-1}$ & $\kappa_i^{-1/2}(1-\kappa_i)^{-1/2}$ \\
Point-mass $(\pi g(\theta_i)+(1-\pi)\delta_0)$ & $\pi h(\lambda_i) + (1-\pi)\delta_0$ & \\
\hline
\end{tabular}
\end{center}
}
\end{frame}



\begin{frame}
\frametitle{Marginal density for $\theta_i$ under horseshoe}

The marginal density for $\theta_i$ under the horseshoe prior is 
\[ p(\theta_i) \propto \int_0^\infty \frac{1}{\lambda_i(1+\lambda_i^2)} e^{-\theta_i^2/2\lambda_i^2} d\lambda_i \]
\pause has no closed form, but the density satisfies the following inequalities for $\theta_i\ne 0$: \pause 
\[ \frac{K}{2}\log\left( 1+\frac{4}{\theta_i^2}\right) < p(\theta_i) < K\log\left(1+\frac{2}{\theta_i^2} \right) \]
where $K=1/(2\pi^3)^{1/2}$. \pause Notice that $p(\theta_i)\to \infty$ as $\theta\to 0$. 

\end{frame}



\begin{frame}[fragile]
\frametitle{Comparison of $\theta_i$ density for some priors}
<<densities, echo=FALSE, fig.width=10, fig.height=5>>=
xmin = c(-3,4); xmax = c(3,7); ylm = c(0.5,0.025)
par(mfrow=c(1,2))
for (i in 1:2) {
  curve(dcauchy(x),xmin[i], xmax[i], ylim=c(0,ylm[i]))
  curve(dexp(abs(x))/2, add=TRUE, lty=2, col=2)
  K = 1/sqrt(2*pi^3)
  curve(K*log(1+4/x^2)/2, add=TRUE, lty=3, n=1001, col=3, lwd=3)
  curve(K*log(1+2/x^2), add=TRUE, lty=3, n=1001, col=3, lwd=3)
  legend("topright", c("Cauchy","Laplace","Horseshoe"), col=c(1,2,3), lty=1:3)
}
@
\end{frame}


\begin{frame}
\frametitle{Comparison of the implied density for $\kappa_i$}
\begin{center}
\includegraphics[page=4, trim=0.85in 4.3in 1in 2.5in, width=0.8\textwidth, clip]{CarvalhoPolsonScott2010}
\end{center}
\end{frame}






\begin{frame}
\frametitle{Posterior mean under LASSO vs Horseshoe}

\begin{center}
\includegraphics[page=4, trim=4in 7in 1in 1in, width=0.8\textwidth, clip]{Carvalhoetal2009}
\end{center}

\end{frame}



\begin{frame}
\frametitle{Interpreting $1-\hat{\kappa}_i$ as posterior probabilities of a signal}

\begin{center}
\includegraphics[page=10, trim=0.7in 7.1in 1in 0.8in, width=\textwidth, clip]{CarvalhoPolsonScott2010}
\end{center}

\end{frame}



\begin{frame}
\frametitle{Squared-error loss for 100w\% signals from $t_\xi(0,3)$ distribution over 500 data sets}

\begin{center}
\includegraphics[page=11, trim=0.7in 7.3in 0.7in 0.8in, width=\textwidth, clip]{CarvalhoPolsonScott2010}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Horseshoe model}

Model from Carvalho 2009:
\[ \begin{array}{rl}
y_i &\stackrel{ind}{\sim} N(\theta_i,\sigma^2) \\
\theta_i &\stackrel{ind}{\sim} N(0,\lambda_i^2\tau^2) \\
\lambda_i & Ca^+(0,1) \\
\tau & Ca^+(0,1) 
\end{array} \]

\vspace{0.2in} \pause 

Model from Carvalho 2010:
\[ \begin{array}{rl}
y_i &\stackrel{ind}{\sim} N(\theta_i,\sigma^2) \\
\theta_i &\stackrel{ind}{\sim} N(0,\lambda_i^2) \\
\lambda_i & Ca^+(0,\tau) \\
\tau & Ca^+(0,\sigma) 
\end{array} \]


\end{frame}



\begin{comment}
\section{Motivation}
\begin{frame}
\frametitle{Independent normal model}
Consider the model 
\[ Y_i \stackrel{ind}{\sim} N(\theta_i,\sigma^2) \]
\pause or, equivalently,
\[ Y \sim N(\theta, \sigma^2\I). \]
where $\theta = (\theta_1,\ldots,\theta_n)'$ is the quantity of interest.

\vspace{0.2in} \pause

The ordinary estimator $\hat{\theta}_{MOM} = \hat{\theta}_{LS} = \hat{\theta}_{MLE} = Y$ \pause is

\begin{itemize}
\item \alert{unbiased} since $E[Y|\theta]=\theta$ \pause and 
\item \alert{admissible}\footnote{no other estimator \alert{dominates} it} when $n\le 2$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{James-Stein estimator}
\small
When $n>2$, the James-Stein estimator 
\[ \hat{\theta}_{JS} = \left( 1-\frac{(n-2)\sigma^2}{||y||^2}\right)y \]
where $||y||^2=y'y$ \pause \alert{dominates} $\hat{\theta}=Y$ and thus the ordinary estimator is inadmissible.

\vspace{0.2in} \pause

If $(n-2)\sigma^2 < ||y||^2$, then the estimator shrinks the ordinary estimator $\hat{\theta}=y$ toward zero.

\vspace{0.2in} \pause 

Comments:
\begin{itemize}[<+->]
\item shrinkage can occur toward any point
\item the James-Stein estimator is inadmissible
\item it works even when $\sigma^2$ is replaced with 
\[ \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (y_i=\overline{y})^2 \]
\end{itemize}
\end{frame}
  

\begin{frame}
\frametitle{Empirical Bayes interpretation}
\small

For simplicity, let $\sigma^2=1$. \pause Now assume $\theta \sim N(0,\lambda^2\I)$ \pause then 
\[ \theta|y,\lambda^2 \sim N(By,B\I) \pause \mbox{ where } B = 1-\frac{1}{1+\lambda^2} \]


\[ y \sim N(0,[1+\lambda^2]\I) \]
and the sum of squares
\[ S = ||y||^2\sim (1+\lambda^2) \chi^2_n \]
so that 
\[ E \left[ \frac{n-2}{S} \right] = \frac{1}{1+\lambda^2}  \]

Thus, the James-Stein estimator replaces $1/(1+\lambda^2)$ with an unbiased estimator of it:

\[ \hat{\theta}_{JS} = \left( 1-\frac{(n-2)}{S}\right)y \]

\end{frame}



\section{Models via scale-mixtures of normals}

\end{comment}


\end{document}